{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "import torch_utils\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global params\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DANet')\n",
    "parser.add_argument('--batch-size', type=int, default=128,\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--cuda', action='store_true', default=True,\n",
    "                    help='enables CUDA training (default: True)')\n",
    "parser.add_argument('--seed', type=int, default=20170220,\n",
    "                    help='random seed (default: 20170220)')\n",
    "parser.add_argument('--infeat-dim', type=int, default=129,\n",
    "                    help='dimension of the input feature (default: 129)')\n",
    "parser.add_argument('--outfeat-dim', type=int, default=20,\n",
    "                    help='dimension of the embedding (default: 20)')\n",
    "parser.add_argument('--threshold', type=float, default=0.9,\n",
    "                    help='the weight threshold (default: 0.9)')\n",
    "parser.add_argument('--seq-len', type=int, default=100,\n",
    "                    help='length of the sequence (default: 100)')\n",
    "parser.add_argument('--log-step', type=int, default=100,\n",
    "                    help='how many batches to wait before logging training status (default: 100)')\n",
    "parser.add_argument('--lr', type=float, default=1e-3,\n",
    "                    help='learning rate (default: 1e-3)')\n",
    "parser.add_argument('--num-layers', type=int, default=4,\n",
    "                    help='number of stacked RNN layers (default: 1)')\n",
    "parser.add_argument('--bidirectional', action='store_true', default=True,\n",
    "                    help='whether to use bidirectional RNN layers (default: True)')\n",
    "parser.add_argument('--val-save', type=str,  default='model.pt',\n",
    "                    help='path to save the best model')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "args.num_direction = int(args.bidirectional)+1\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "else:\n",
    "    kwargs = {}\n",
    "\n",
    "# training and validation datast path\n",
    "training_data_path = 'your_path_to_training_set'\n",
    "validation_data_path = 'your_path_to_validation_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data loaders\n",
    "\n",
    "train_loader = DataLoader(data_utils.WSJDataset(training_data_path), \n",
    "                          batch_size=args.batch_size, \n",
    "                          shuffle=True, \n",
    "                          **kwargs)\n",
    "\n",
    "validation_loader = DataLoader(data_utils.WSJDataset(validation_data_path), \n",
    "                          batch_size=args.batch_size, \n",
    "                          shuffle=False, \n",
    "                          **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "class DANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANet, self).__init__()\n",
    "        \n",
    "        self.rnn = torch_utils.MultiRNN('LSTM', args.infeat_dim, 300, \n",
    "                                           num_layers=args.num_layers, \n",
    "                                           bidirectional=args.bidirectional)\n",
    "        self.FC = torch_utils.FCLayer(600, args.infeat_dim*args.outfeat_dim, nonlinearity='tanh')\n",
    "        \n",
    "        self.infeat_dim = args.infeat_dim\n",
    "        self.outfeat_dim = args.outfeat_dim\n",
    "        self.eps = 1e-8\n",
    "        \n",
    "    def forward(self, input, ibm, weight, hidden):\n",
    "        \"\"\"\n",
    "        input: the input feature; \n",
    "            shape: (B, T, F)\n",
    "            \n",
    "        ibm: the ideal binary mask used for calculating the \n",
    "            ideal attractors; \n",
    "            shape: (B, T*F, nspk)\n",
    "            \n",
    "        weight: the binary energy threshold matrix for masking \n",
    "            out T-F bins; \n",
    "            shape: (B, T*F, 1)\n",
    "            \n",
    "        hidden: the initial hidden state in the LSTM layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        seq_len = input.size(1)\n",
    "        \n",
    "        # generate the embeddings (V) by the LSTM layers\n",
    "        LSTM_output, hidden = self.rnn(input, hidden)\n",
    "        LSTM_output = LSTM_output.contiguous().view(-1, LSTM_output.size(2))  # B*T, H \n",
    "        V = self.FC(LSTM_output)  # B*T, F*K\n",
    "        V = V.view(-1, seq_len*self.infeat_dim, self.outfeat_dim)  # B, T*F, K\n",
    "        \n",
    "        # calculate the ideal attractors\n",
    "        # first calculate the source assignment matrix Y\n",
    "        Y = ibm * weight.expand_as(ibm) # B, T*F, nspk\n",
    "        \n",
    "        # attractors are the weighted average of the embeddings\n",
    "        # calculated by V and Y\n",
    "        V_Y = torch.bmm(torch.transpose(V, 1,2), Y)  # B, K, nspk\n",
    "        sum_Y = torch.sum(Y, 1, keepdim=True).expand_as(V_Y)  # B, K, nspk\n",
    "        attractor = V_Y / (sum_Y + self.eps)  # B, K, 2\n",
    "        \n",
    "        # calculate the distance bewteen embeddings and attractors\n",
    "        # and generate the masks\n",
    "        dist = V.bmm(attractor)  # B, T*F, nspk\n",
    "        mask = F.softmax(dist, dim=2)  # B, T*F, nspk\n",
    "        \n",
    "        return mask, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return self.rnn.init_hidden(batch_size)\n",
    "        \n",
    "    \n",
    "def objective(mixture, wfm, estimated_mask):\n",
    "    \"\"\"\n",
    "    MSE as the training objective. The mask estimation loss is calculated.\n",
    "    You can also change it into the spectrogram estimation loss, which is \n",
    "    to calculate the MSE between the clean source spectrograms and the \n",
    "    masked mixture spectrograms.\n",
    "    \n",
    "    mixture: the spectrogram of the mixture;\n",
    "        shape: (B, T, F)\n",
    "        \n",
    "    wfm: the target masks, which are the wiener-filter like masks here;\n",
    "        shape: (B, T*F, nspk)\n",
    "    \n",
    "    estimated_mask: the estimated masks generated by the network;\n",
    "        shape: (B, T*F, nspk)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = mixture.expand(mixture.size(0), mixture.size(1), wfm.size(2)) * (wfm - estimated_mask)\n",
    "    loss = loss.view(-1, loss.size(1)*loss.size(2))\n",
    "    \n",
    "    return torch.mean(torch.sum(torch.pow(loss, 2), 1))\n",
    "    \n",
    "# define the model and the optimizer\n",
    "model = DANet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "current_lr = args.lr\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler  = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training and validation\n",
    "\n",
    "def train(epoch):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # data loading\n",
    "    # see data_utils.py for dataloader details\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        # batch_infeat is the input feature\n",
    "        batch_infeat = Variable(data[0]).contiguous()\n",
    "        \n",
    "        # wiener-filter like mask as the training target\n",
    "        batch_wfm = Variable(data[1]).contiguous()\n",
    "        \n",
    "        # spectrogram of mixture, used in objective\n",
    "        batch_mix = Variable(data[2]).contiguous()\n",
    "        \n",
    "        # ideal binary mask as the ideal source assignment\n",
    "        # used during the calculation of attractors\n",
    "        batch_ibm = Variable(data[3]).contiguous()\n",
    "        \n",
    "        # energy threshold matrix calculated from the mixture spectrogram\n",
    "        batch_weight = Variable(data[4]).contiguous()\n",
    "        \n",
    "        if args.cuda:\n",
    "            batch_infeat = batch_infeat.cuda()  # B, T, F\n",
    "            batch_wfm = batch_wfm.cuda()  # B, T*F, nspk\n",
    "            batch_mix = batch_mix.cuda()  # B, T, F\n",
    "            batch_ibm = batch_ibm.cuda()  # B, T*F, nspk\n",
    "            batch_weight = batch_weight.cuda()  # B, T*F, 1\n",
    "        \n",
    "        # training\n",
    "        hidden = model.init_hidden(batch_infeat.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        estimated_mask, hidden = model(batch_infeat, batch_ibm, batch_weight, hidden)\n",
    "        \n",
    "        loss = objective(batch_mix, batch_wfm, estimated_mask)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # output logs\n",
    "        if (batch_idx+1) % args.log_step == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} |'.format(\n",
    "                epoch, batch_idx+1, len(train_loader),\n",
    "                elapsed * 1000 / (batch_idx+1), train_loss / (batch_idx+1)))\n",
    "    \n",
    "    train_loss /= (batch_idx+1)\n",
    "    print('-' * 99)\n",
    "    print('    | end of training epoch {:3d} | time: {:5.2f}s | training loss {:5.2f} |'.format(\n",
    "            epoch, (time.time() - start_time), train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "        \n",
    "def validate(epoch):\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    validation_loss = 0.\n",
    "    \n",
    "    # data loading\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        batch_infeat = Variable(data[0]).contiguous()\n",
    "        batch_wfm = Variable(data[1]).contiguous()\n",
    "        batch_mix = Variable(data[2]).contiguous()\n",
    "        batch_ibm = Variable(data[3]).contiguous()\n",
    "        batch_weight = Variable(data[4]).contiguous()\n",
    "        \n",
    "        if args.cuda:\n",
    "            batch_infeat = batch_infeat.cuda()\n",
    "            batch_wfm = batch_wfm.cuda()\n",
    "            batch_mix = batch_mix.cuda()\n",
    "            batch_ibm = batch_ibm.cuda()\n",
    "            batch_weight = batch_weight.cuda()\n",
    "        \n",
    "        # mask estimation\n",
    "        with torch.no_grad():\n",
    "            hidden = model.init_hidden(batch_infeat.size(0))\n",
    "            estimated_mask, hidden = model(batch_infeat, batch_ibm, batch_weight, hidden)\n",
    "        \n",
    "            loss = objective(batch_mix, batch_wfm, estimated_mask)\n",
    "            validation_loss += loss.data.item()\n",
    "    \n",
    "    validation_loss /= (batch_idx+1)\n",
    "    print('    | end of validation epoch {:3d} | time: {:5.2f}s | validation loss {:5.2f} |'.format(\n",
    "            epoch, (time.time() - start_time), validation_loss))\n",
    "    print('-' * 99)\n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "decay_cnt = 0\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    model.cuda()\n",
    "    training_loss.append(train(epoch))\n",
    "    validation_loss.append(validate(epoch))\n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "        print('-' * 99)\n",
    "    if validation_loss[-1] == np.min(validation_loss):\n",
    "        # save current best model\n",
    "        with open(args.val_save, 'wb') as f:\n",
    "            torch.save(model.cpu().state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "            print('-' * 99)\n",
    "    decay_cnt += 1\n",
    "    # lr decay\n",
    "    if np.min(training_loss) not in training_loss[-3:] and decay_cnt >= 3:\n",
    "        scheduler.step()\n",
    "        decay_cnt = 0\n",
    "        print('      Learning rate decreased.')\n",
    "        print('-' * 99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
